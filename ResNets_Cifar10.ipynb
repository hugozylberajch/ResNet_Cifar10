{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2: Residual Networks for Image Classification\n",
    "### INF581: Advanced Topics in Arti\f",
    "cial Intelligence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cifar10 is a classic dataset for deep learning, consisting of 32x32 images split in 50k train images and 10k test images, belonging to 10 different classes (plane, car, bird, cat, deer, dog, frog, horse, ship, truck). Cifar10 resembles MNIST â€” both have 10 classes and tiny images. However, while getting 90% accuracy on MNIST is trivial, getting 90% on Cifar10 requires serious work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is then to implement a Residual Network (ResNet), using pytorch library. (see figure at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: \n",
    "The torchvision package allows you to introduce data augmentation layers to your data. Create two dataloaders for training and testing data using:\n",
    "1. zero padding of 4 pixels and cropping randomly a 32x32 patch\n",
    "2. Random Horizontal Flip \n",
    "\n",
    "as a data augmentation method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "net_best_acc = 0  # best test accuracy on the self supervised task\n",
    "net_start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "exclude_list=[0,1,3,4,5,6,7,8] # keep only the 'bird' and the 'ship' class\n",
    "# exclude_list=[] # uncomment this line to include all data\n",
    "\n",
    "class CIFAR10_INF581(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    Custom Dataset build on top of CIFAR10\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, exclude_list=[], **kwargs):\n",
    "        super(CIFAR10_INF581, self).__init__(*args, **kwargs)\n",
    "\n",
    "        if exclude_list == []:\n",
    "            return\n",
    "\n",
    "        targets = np.array(self.targets)\n",
    "        exclude = np.array(exclude_list).reshape(1, -1)\n",
    "        mask = ~(targets.reshape(-1, 1) == exclude).any(axis=1)\n",
    "        self.data = self.data[mask]\n",
    "        self.targets = targets[mask].tolist()\n",
    "\n",
    "trainset = CIFAR10_INF581(root='./data', train=True, download=True, exclude_list=exclude_list, transform=transform_train)\n",
    "testset = CIFAR10_INF581(root='./data', train=False, download=True, exclude_list=exclude_list, transform=transform_test)\n",
    "\n",
    "# Original dataset\n",
    "# trainset = CIFAR10(root='./data', train=True, download=True, exclude_list=exclude_list, transform=transform_train)\n",
    "# testset = CIFAR10(root='./data', train=False, download=True, exclude_list=exclude_list, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,shuffle=False,  num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "Run the following script to visualise some of the data and familiarise yourself with the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is ready we need the following train and test functions, as well as the learning rate adjust function for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr, rate=0.2, adjust_frequency=30):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by <rate> every <adjust_frequency> epochs\"\"\"\n",
    "    lr = init_lr * (rate ** (epoch // adjust_frequency))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch,net=None):\n",
    "    global lr, optimizer\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    adjust_learning_rate(optimizer, epoch, lr, rate=0.2, adjust_frequency=30)\n",
    "\n",
    "    net.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, data in enumerate(tqdm(trainloader),0):\n",
    "        inputs, targets = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()*inputs.size(0)\n",
    "        count+=inputs.size(0)\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # if batch_idx>2:\n",
    "            # break\n",
    "\n",
    "    print('Epoch %d, Train, Loss: %.3f, Acc: %.3f' % (epoch, total_loss/count , 100.*correct/total))\n",
    "\n",
    "\n",
    "def test(epoch,net=None):\n",
    "    global net_best_acc\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(testloader)):\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()*inputs.size(0)\n",
    "            count+=inputs.size(0)\n",
    "        \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print('Epoch %d, Test,  Loss: %.3f, Acc: %.3f' % (epoch, total_loss/count, 100.*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "#### MLP\n",
    "Pytorch already offers multiple predefined modules that we can use to build neural networks. In case the available modules do not satisfy our needs, we can inherit the base module class and define our own module. \n",
    "\n",
    "In the following example, we create a simple MLP with predefined modules. We found that if the input is not shaped in a friendly format we can reshape it using our custom module Flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building MLP...')\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3*32*32,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,10)\n",
    ")\n",
    "mlp = mlp.to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# lr and optimizer for the self supervised task\n",
    "lr=0.1\n",
    "\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "n_epochs_net = 10\n",
    "print('Training mlp for {} epochs'.format( n_epochs_net))\n",
    "for epoch in range(net_start_epoch, n_epochs_net):\n",
    "    train(epoch,net=mlp)\n",
    "    test(epoch,net=mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "We used the same basic principles as in the MLP to define a ResNet. As a task try to define the blocks of our ResNet by filling the missing code.\n",
    "The ResNet implementation is composed of \"layers\" which maintain the same number of input and output planes.\n",
    "Each layer is composed from a a cascade of a number of blocks. A ResNet block is the building block of a ResNet and it's defined as:\n",
    "$$\n",
    "g\\left(BN(W_2\\star g\\left(BN\\left(W_1 \\star x \\right)\\right) + x\\right)\n",
    "$$\n",
    "where\n",
    "$\n",
    "W_1\\in\\mathbb{R}^{in\\_planes \\times planes \\times 3 \\times 3}\n",
    "$,\n",
    "$\n",
    "W_2\\in\\mathbb{R}^{planes \\times planes \\times 3 \\times 3}\n",
    "$,\n",
    "$g\\left(x\\right)= ReLU\\left(x\\right)$  and \n",
    "$BN $ stands for Batch Normalisation.\n",
    "##Commented out solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the blocks together to define a ResNet module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        sp = 4       # Increase the number of features to increase performance\n",
    "        self.in_planes = sp\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, sp, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.layer1 = self._make_layer(block, sp, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 2*sp, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 4*sp, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, 4*sp, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(4*sp*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, return_last_layer=False, return_both = False,second_head=False):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out) #Increase the number of layers to increase performance\n",
    "        if return_last_layer:\n",
    "            return out\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out_pre = out.view(out.size(0), -1)\n",
    "        out = self.linear(out_pre)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet14():\n",
    "    return ResNet(BasicBlock, [2,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building MLP...')\n",
    "\n",
    "net= ResNet14()\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# lr and optimizer for the self supervised task\n",
    "lr=0.1\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "n_epochs_net = 10\n",
    "print('Training mlp for {} epochs'.format( n_epochs_net))\n",
    "for epoch in range(net_start_epoch, n_epochs_net):\n",
    "    train(epoch,net=net)\n",
    "    test(epoch,net=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint \n",
    "The network topology of the desired network is sketched in the following graph:\n",
    "\n",
    "![title](visualisation.png)\n",
    "\n",
    "you can visualise the graph of your network using, the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "random_input=torch.randn(32,3,32,32)\n",
    "out=net(random_input)\n",
    "dot=make_dot(out.mean())\n",
    "dot.render(\"visualisation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
